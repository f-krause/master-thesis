# PTRnet - nucleotide level model

# Training
epochs: 200
save_freq: 1
val_freq: 1
warmup: 0
batch_size: 64
#batch_size: 64  # for pretrain
num_workers: 1
masked_tokens: 0.15  # % of randomly masked tokens during pretraining

# Model
model: "ptrnet"
dim_embedding_tissue: 64  # max 29 tissues
dim_embedding_token: 64  # 64 codons or 4 nucleotides/
predictor_hidden_dim: 32  # hidden size of the output layer
predictor_dropout: 0.3  # dropout rate of the output layer

# RiboNN
base_channels: 64
num_blocks: 10

# PTRnet
d_state: 16   # SSM state expansion factor
d_conv: 3      # Local convolution width
expand: 2      # Block expansion factor
num_layers: 8  # number of sequential mamba blocks

# Optimizer
optimizer:
  name: adam  # sgd, adam, ranger
  lr: 0.001
#  lr: 0.0001  # for pretrain
  weight_decay: 0.01

# Scheduler (Cosine Annealing Warm Restarts)
lr_scheduler:
  enable: False
  warmup: 10
#  warmup: 20  # for pretrain
  reset_epochs: 5
  T_mult: 2
  min_lr: 1e-7