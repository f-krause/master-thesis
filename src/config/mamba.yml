# Training
epochs: 200
save_freq: 3
val_freq: 1
warmup: 0
batch_size: 64
num_workers: 0  # TODO seems faster than 1

# Model
model: "mamba"
dim_embedding_token: 8  # Embedding dimension for tokens
tissue_embedding_dim: 16  # Embedding dimension for tissues
dim_embedding_tissue: 16  # Embedding dimension for tissues
predictor_hidden_dim: 64
predictor_dropout: 0.1

# Mamba2 Config
# Requirement: (dim_embedding_token + dim_embedding_tissue) * expand / headdim = multiple of 8
d_state: 64   # SSM state expansion factor
d_conv: 4     # Local convolution width
expand: 2     # Block expansion factor
headdim: 32   # only for mamba2

# Optimizer
optimizer:
  name: adam
  lr: 0.0001
  weight_decay: 0.1  # ranger only
