#subproject: "mamba/v1_codon_full_data"
#subproject: "bin/dev_mamba/v1_codon_full_data"
subproject: "dev_mamba/delete_me"

# Training
gpu_id: 1
epochs: 50
save_freq: 5
val_freq: 1
warmup: 0
batch_size: 32
num_workers: 0  # TODO seems faster than 1

# Model
model: "mamba"
dim_embedding_token: 64  # Embedding dimension for tokens
tissue_embedding_dim: 64  # Embedding dimension for tissues
predictor_hidden_dim: 64
predictor_dropout: 0.1

# Mamba2 Config
# Requirement: (dim_embedding_token + tissue_embedding_dim) * expand / headdim = multiple of 8
d_state: 64   # SSM state expansion factor
d_conv: 4     # Local convolution width
expand: 2     # Block expansion factor
headdim: 32   # only for mamba2

# Optimizer
optimizer:
  name: adam
  lr: 0.0001
  weight_decay: 0.1  # ranger only
