# Paths
subproject: "dev_mamba/v2_test_150"
train_data_file: "dev_train_data_1000_full_length.pkl" # dev_train_data_1000.pkl or dev_train_data_1000_full_length.pkl
project_path: ""  # optional, project path with data and model weights
log_file_path: "" # optional

# General
folding_algorithm: "viennarna"  # options: viennarna, linearfold
nr_folds: 1  # if 1, will take 20% of the data as validation set
gpu_id: 0
seed: 2024
max_seq_length: 3000  # Should match context_length in Mamba2

# Training
epochs: 100
save_freq: 10
val_freq: 5
warmup: 0
batch_size: 32
num_workers: 0  # TODO seems faster than 1
final_evaluation: True
clean_up_weights: True

# Model
model: "mamba"
dim_embedding_token: 64  # Embedding dimension for tokens
tissue_embedding_dim: 64  # Embedding dimension for tissues
out_hidden_size: 64

# Mamba2 Config
# Requirement: (dim_embedding_token + tissue_embedding_dim) * expand / headdim = multiple of 8
d_state: 64   # SSM state expansion factor
d_conv: 4     # Local convolution width
expand: 2     # Block expansion factor
headdim: 32   # only for mamba2

# Optimizer
optimizer:
  name: adam
  lr: 0.0001
