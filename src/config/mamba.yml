#subproject: "dev_mamba/v2_test_150"
subproject: "dev_mamba/delete_me"

# Training
gpu_id: 0
epochs: 100
save_freq: 10
val_freq: 10
warmup: 0
batch_size: 32
num_workers: 0  # TODO seems faster than 1

# Model
model: "mamba"
dim_embedding_token: 64  # Embedding dimension for tokens
tissue_embedding_dim: 64  # Embedding dimension for tissues
out_hidden_size: 64

# Mamba2 Config
# Requirement: (dim_embedding_token + tissue_embedding_dim) * expand / headdim = multiple of 8
d_state: 64   # SSM state expansion factor
d_conv: 4     # Local convolution width
expand: 2     # Block expansion factor
headdim: 32   # only for mamba2

# Optimizer
optimizer:
  name: adam
  lr: 0.0001
