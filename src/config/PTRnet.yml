# Training
epochs: 200
save_freq: 1
val_freq: 1
warmup: 0
batch_size: 32
num_workers: 1
masked_tokens: 0.15  # % of randomly masked tokens during pretraining

# Model
model: "ptrnet"
dim_embedding_tissue: 128  # max 29 tissues
dim_embedding_token: 128  # 64 codons or 4 nucleotides/
predictor_hidden_dim: 32  # hidden size of the output layer
predictor_dropout: 0.1  # dropout rate of the output layer

# PTRnet
d_state: 32   # SSM state expansion factor
d_conv: 3      # Local convolution width
expand: 2      # Block expansion factor
num_layers: 2  # number of sequential mamba blocks

# Optimizer
optimizer:
  name: adam  # sgd, adam, ranger
  lr: 0.001

# Scheduler (Cosine Annealing Warm Restarts)
lr_scheduler:
  reset_epochs: 5
  T_mult: 2
  min_lr: 1e-6