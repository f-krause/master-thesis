# Paths
subproject: "dev_transformer/v2_test_150"
project_path: ""  # optional, project path with data and model weights
log_file_path: "" # optional

# General
folding_algorithm: "viennarna"  # options: viennarna, linearfold
nr_folds: 1  # if 1, will take 20% of the data as validation set
gpu_id: 0
seed: 2024
max_seq_length: 3000  # Should match context_length in Transformer

# Training
epochs: 150
save_freq: 25
val_freq: 10
warmup: 0
batch_size: 32
num_workers: 1
final_evaluation: True

# Model
model: "transformer"
dim_embedding_token: 32  # Embedding dimension for tokens
tissue_embedding_dim: 32  # Embedding dimension for tissues
out_hidden_size: 64
output: 1

# Transformer Config
num_heads: 8
num_layers: 6
dim_feedforward: 256
dropout: 0.1
activation: "gelu"

# Optimizer
optimizer:
  name: adam
  lr: 0.0001
