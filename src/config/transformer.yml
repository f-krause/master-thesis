# Training
epochs: 100
save_freq: 1
val_freq: 1
warmup: 0
batch_size: 64
num_workers: 1

# Model
model: "transformer"
# probably 32 better for complex models
dim_embedding_tissue: 64  # max 29 tissues
dim_embedding_token: 64  # 64 codons or 4 nucleotides
predictor_hidden_dim: 64
predictor_dropout: 0.1

# Transformer Config
num_heads: 2
num_layers: 12
dim_feedforward: 256
dropout: 0.2
activation: "gelu"

# Optimizer
optimizer:
  name: adam
  lr: 0.0001
  weight_decay: 0.1  # ranger only
