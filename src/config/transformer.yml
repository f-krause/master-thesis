# Training
epochs: 200
save_freq: 3
val_freq: 1
warmup: 0
batch_size: 64
num_workers: 1

# Model
model: "transformer"
dim_embedding_tissue: 8  # max 29 tissues
dim_embedding_token: 16  # 64 codons or 4 nucleotides
predictor_hidden_dim: 64
predictor_dropout: 0.1

# Transformer Config
num_heads: 8
num_layers: 6
dim_feedforward: 256
dropout: 0.2
activation: "gelu"

# Optimizer
optimizer:
  name: adam
  lr: 0.001
  weight_decay: 0.1  # ranger only
