# Training
epochs: 100
save_freq: 1
val_freq: 1
warmup: 0
batch_size: 64
num_workers: 1

# Model
model: "transformer"
dim_embedding_tissue: 32  # max 29 tissues
dim_embedding_token: 32  # 64 codons or 4 nucleotides
predictor_hidden_dim: 128
predictor_dropout: 0.1
random_reverse: True

# Transformer config
num_heads: 4
num_layers: 12
dim_feedforward: 128
dropout: 0.2
activation: "gelu"

# Optimizer
optimizer:
  name: adam
  lr: 0.00010919470279424945  # Note: lr was decreased by x10 as Optuna set lr lead to oscillating losses
  weight_decay: 0
