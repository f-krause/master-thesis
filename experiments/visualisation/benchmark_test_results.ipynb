{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager as fm\n",
    "\n",
    "import vis_utils\n",
    "from custom_colors import *\n",
    "\n",
    "font_path = r'C:\\Users\\Felix\\AppData\\Local\\Microsoft\\Windows\\Fonts\\SourceSansPro-Regular.ttf'\n",
    "fm.fontManager.addfont(font_path)\n",
    "source_sans_pro = fm.FontProperties(fname=font_path)\n",
    "\n",
    "plt.rcParams['font.family'] = source_sans_pro.get_name()"
   ],
   "id": "dc445d8fe0f8bb0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "STORE = False\n",
    "# STORE = True"
   ],
   "id": "c7a121ef1ff03d25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_raw = pd.read_csv(\"data/runs-17_19_25-05-May-25.csv\")\n",
    "df_raw = df_raw.drop([0], axis=0)"
   ],
   "id": "9fb248e8ad4e85e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_raw",
   "id": "af42d4e6411b1740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for l in df_raw.columns:\n",
    "    if \"ccur\" in l:\n",
    "        print(l)"
   ],
   "id": "df769ee82d0e3b0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Other scores\n",
    "df_raw[[\"model_config.model\", \"AUC_val\", \"F1_val\", \"Precision_val\", \"Recall_val\", \"Accuracy_val\"]]"
   ],
   "id": "36521799aca72fcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Other scores\n",
    "df_raw[[\"model_config.model\", \"AUC_val\", \"F1_val\", \"Precision_val\", \"Recall_val\", \"Accuracy_val\"]]"
   ],
   "id": "17bcdfe98085cd22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = df_raw[[\"experiment\", 'AUC_val', 'AUC_train', 'training_time_min', 'avg_epoch_time', 'nr_params', 'nr_flops', \"best_epoch\", \"model_config.model\", \"F1_val\", \"Precision_val\", \"Recall_val\", \"Accuracy_val\"]]",
   "id": "1d1436e9239d8c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.loc[:, \"model_config.model\"] = [\"baseline_freq\" if \"freq\" in name else model.strip(\"\\\"\") for name, model in zip(df[\"experiment\"], df[\"model_config.model\"])]\n",
    "df = df.rename(columns={'model_config.model': 'model', \"avg_epoch_time\": \"avg_epoch_min\"})\n",
    "df = df.drop(\"experiment\", axis=1)"
   ],
   "id": "732603bd5d5aa6fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cols_to_convert = [col for col in df.columns if col != 'model']\n",
    "df.loc[:, cols_to_convert] = df[cols_to_convert].astype(float)"
   ],
   "id": "19073adbb634ad8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.columns",
   "id": "e6b8769030d2a930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# custom colnames\n",
    "# \"val\" scores are actually test, because the set to evaluate for this data was test\n",
    "df.columns = [\"test AUC\", \"train AUC\", \"train time (min.)\", \"avg epoch time (min.)\", \"# parameters\", \"# FLOPS\", \"best epoch\", \"model\", \"test F1 Score\", \"test Precision\", \"test Recall\", \"test Accuracy\"]"
   ],
   "id": "36bbeebfa9953f76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Adding validation AUC (tuning) results\n",
    "\n",
    "    #  \"MLP Baseline\": {\n",
    "    #     \"val AUC\": 0.6686054843599283,\n",
    "    # \"MLP Freq\": {\n",
    "    #     \"val AUC\": 0.7248128243994304,\n",
    "    # \"CNN\": {\n",
    "    #     \"val AUC\": 0.6908984428827338,\n",
    "    # \"LegNet\": {\n",
    "    #     \"val AUC\": 0.710789582472096,\n",
    "    # \"RiboNN\": {\n",
    "    #     \"val AUC\": 0.6990592990675669,\n",
    "    # \"LSTM\": {\n",
    "    #     \"val AUC\": 0.6774309861742686,\n",
    "    # \"GRU\": {\n",
    "    #     \"val AUC\": 0.6836840292131734,\n",
    "    # \"xLSTM\": {\n",
    "    #     \"val AUC\": 0.6890615957007028,\n",
    "    # \"Transformer\": {\n",
    "    #     \"val AUC\": 0.680868127325341,\n",
    "    # \"Mamba\": {\n",
    "    #     \"val AUC\": 0.686000643057278,\n",
    "\n",
    "df_val = pd.DataFrame(\n",
    "    {\n",
    "        \"val AUC (tuning)\": [\n",
    "            0.6686054843599283,\n",
    "            0.7248128243994304,\n",
    "            0.6908984428827338,\n",
    "            0.710789582472096,\n",
    "            0.6990592990675669,\n",
    "            0.6774309861742686,\n",
    "            0.6836840292131734,\n",
    "            0.6890615957007028,\n",
    "            0.680868127325341,\n",
    "            0.686000643057278\n",
    "        ]\n",
    "    },\n",
    "    index=[\"baseline\", \"baseline_freq\", \"cnn\", \"LegNet\", \"RiboNN\", \"lstm\", \"gru\", \"xlstm\", \"transformer\", \"mamba\"]\n",
    ")\n",
    "\n",
    "# merge with df (on model column\n",
    "df = df.merge(df_val, left_on=\"model\", right_index=True) #, suffixes=(\"\", \"_val\")"
   ],
   "id": "6b3ffb291d0413df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Results RFC (train+val, and test)\n",
    "\n",
    "# RandomForestClassifier\n",
    "# Training Time (s): 18.88788938522339\n",
    "\n",
    "# Train Accuracy: 1.0\n",
    "# Train Precision: 1.0\n",
    "# Train Recall: 1.0\n",
    "# Train F1: 1.0\n",
    "\n",
    "# Test ROC AUC: 0.6681921274260418\n",
    "# Test Accuracy: 0.6119725928597187\n",
    "# Test Precision: 0.6097212294496068\n",
    "# Test Recall: 0.6167751265365148\n",
    "# Test F1: 0.6132278936017254\n",
    "\n",
    "# train, val data results\n",
    "# Validation ROC AUC: 0.6634033806439759\n",
    "# Validation Accuracy: 0.6123348017621145\n",
    "# Validation Precision: 0.6101131071190952\n",
    "# Validation Recall: 0.6216949152542373\n",
    "# Validation F1: 0.615849563465413\n",
    "\n",
    "\n",
    "rfc_data = {\n",
    "    \"test AUC\": 0.6681921274260418,\n",
    "    \"test F1 Score\": 0.6132278936017254,\n",
    "    \"test Precision\": 0.6097212294496068,\n",
    "    \"test Recall\": 0.6167751265365148,\n",
    "    \"val AUC (tuning)\": 0.6634033806439759,\n",
    "    \"train AUC\": np.nan,\n",
    "    \"train time (min.)\": 18.887889 / 60,\n",
    "    \"avg epoch time (min.)\": np.nan,\n",
    "    \"# parameters\": np.nan,\n",
    "    \"# FLOPS\": np.nan,\n",
    "    \"best epoch\": np.nan,\n",
    "    \"model\": \"RFC (freq)\"\n",
    "}\n",
    "\n",
    "rfc_df = pd.DataFrame(rfc_data, index=[0])\n",
    "\n",
    "df = pd.concat([df, rfc_df], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ],
   "id": "ef995e0b724d4495"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.index = df.model\n",
    "df = df.rename(index={\n",
    "    \"baseline\": \"MLP (baseline)\",\n",
    "    \"baseline_freq\": \"MLP (freq)\",\n",
    "    \"cnn\": \"CNN\",\n",
    "    \"gru\": \"GRU\",\n",
    "    \"lstm\": \"LSTM\",\n",
    "    \"mamba\": \"Mamba\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"xlstm\": \"xLSTM\",\n",
    "    \"LEGnet\": \"LEGnet\",\n",
    "    \"RFC (freq)\": \"RFC (freq)\"\n",
    "})"
   ],
   "id": "9c2a41dcb73338e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df",
   "id": "586afb643623ed36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_table = df.copy()[['test AUC', 'val AUC (tuning)', 'train AUC', 'train time (min.)', 'avg epoch time (min.)',\n",
    "       '# parameters', '# FLOPS', 'best epoch']]\n",
    "df_table.sort_values(by=\"test AUC\", ascending=False, inplace=True)\n",
    "#df_table.reset_index(inplace=True)\n",
    "df_table = df_table.round(5)\n",
    "df_table[[\"# parameters\", \"# FLOPS\"]] = df_table[[\"# parameters\", \"# FLOPS\"]].astype(pd.Int64Dtype())\n",
    "df_table"
   ],
   "id": "5deec4c1d8d147a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def transparent_nan(val):\n",
    "    if pd.isnull(val) or val is pd.NA:\n",
    "        return 'background-color: white; color: white;'\n",
    "    return ''\n",
    "\n",
    "df_table.index.name = None\n",
    "\n",
    "styled_df = (\n",
    "    df_table.style\n",
    "    .background_gradient(subset=['test AUC'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['val AUC (tuning)'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['train AUC'], cmap='Greens')  # Color scale for 'train AUC'\n",
    "    .background_gradient(subset=['train time (min.)'], cmap='Reds')  # Color scale for 'train time'\n",
    "    .background_gradient(subset=['# parameters'], cmap='Reds')  # Color scale for '# Parameters'\n",
    "    .background_gradient(subset=['# FLOPS'], cmap='Reds')  # Color scale for '# Parameters'\n",
    "    .background_gradient(subset=['avg epoch time (min.)'], cmap='Reds')  # Color scale for '# Parameters'\n",
    "    .background_gradient(subset=['best epoch'], cmap='Reds')  # Color scale for '# Parameters'\n",
    "    .map(transparent_nan)\n",
    "    .format(precision=4)\n",
    ")\n",
    "\n",
    "styled_df"
   ],
   "id": "3a3e732bb89b9e61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# write to html\n",
    "if STORE:\n",
    "    html_path = os.path.join(os.getenv(\"OUTPUT_DIR\"), \"benchmark_results.html\")\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(styled_df.to_html())"
   ],
   "id": "bdaf96634e96df51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Table of more Metrics",
   "id": "2d2a747860fae9c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_table2 = df.copy()[['train AUC', 'val AUC (tuning)', 'test AUC', \"test F1 Score\", \"test Precision\", \"test Recall\"]]\n",
    "df_table2.sort_values(by=\"test AUC\", ascending=False, inplace=True)\n",
    "#df_table.reset_index(inplace=True)\n",
    "df_table2 = df_table2.round(5)\n",
    "df_table2"
   ],
   "id": "93f2ac321d36dca7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add best PTRnet results\n",
    "# TODO"
   ],
   "id": "278e29186d3c008a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def transparent_nan(val):\n",
    "    if pd.isnull(val) or val is pd.NA:\n",
    "        return 'background-color: white; color: white;'\n",
    "    return ''\n",
    "\n",
    "df_table2.index.name = None\n",
    "\n",
    "styled_df = (\n",
    "    df_table2.style\n",
    "    .background_gradient(subset=['train AUC'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['val AUC (tuning)'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['test AUC'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['test F1 Score'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['test Precision'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .background_gradient(subset=['test Recall'], cmap='Greens')  # Color scale for 'test AUC'\n",
    "    .map(transparent_nan)\n",
    "    .format(precision=4)\n",
    "    # make columns equally wide\n",
    "    .set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [('width', '400px')]\n",
    "    }])\n",
    ")\n",
    "\n",
    "styled_df"
   ],
   "id": "3ac75cd0cbe7402e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# write to html\n",
    "if STORE:\n",
    "    html_path = os.path.join(os.getenv(\"OUTPUT_DIR\"), \"benchmark_results_metrics.html\")\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(styled_df.to_html())"
   ],
   "id": "a1df45a379373cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results Barplot",
   "id": "36bf20f563d0abb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_barplot = df_table.sort_values(by=\"test AUC\", ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bar_height = 0.35\n",
    "y = range(len(df))\n",
    "\n",
    "# Bars with spacing\n",
    "ax.barh([i + bar_height / 2 for i in y], df_barplot[\"test AUC\"], height=bar_height, label='Test', color=blue_shades[4])\n",
    "ax.barh([i - bar_height / 2 for i in y], df_barplot[\"val AUC (tuning)\"], height=bar_height, label='Validation', color=blue_shades[0])\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(df_barplot.index)\n",
    "ax.set_xlabel(\"AUC\")\n",
    "ax.set_title(\"Benchmarking Results\")\n",
    "ax.legend()\n",
    "ax.set_xlim(0.5, max(df_barplot[\"val AUC (tuning)\"].max(), df_barplot[\"test AUC\"].max()) * 1.035)\n",
    "plt.tight_layout()\n",
    "\n",
    "if STORE:\n",
    "    plt.savefig(os.getenv(\"OUTPUT_DIR\") + f\"/benchmark_barplot.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "33323e8a634d755b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simulating AUC Scores for Random Majority Classifier",
   "id": "986e67e798b3d682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Simulated dataset\n",
    "n_samples = 100\n",
    "y_true = np.array([0]*90 + [1]*10)  # 90 negative, 10 positive\n",
    "y_pred_majority_class = np.array([0]*100)  # Predict all as majority class (0)\n",
    "y_scores_constant = np.array([0]*50 + [0.2]*50)  # Constant score for all (e.g., naive predictor)\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_true, y_scores_constant)\n",
    "pr_auc = average_precision_score(y_true, y_scores_constant)\n",
    "f1 = f1_score(y_true, y_pred_majority_class, zero_division=0)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred_majority_class)\n",
    "precision = precision_score(y_true, y_pred_majority_class, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred_majority_class, zero_division=0)\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC AUC', 'PR AUC', 'F1 Score', 'Balanced Accuracy', 'Precision', 'Recall'],\n",
    "    'Value': [\n",
    "        np.mean(y_pred_majority_class == y_true),\n",
    "        roc_auc,\n",
    "        pr_auc,\n",
    "        f1,\n",
    "        balanced_acc,\n",
    "        precision,\n",
    "        recall\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "metrics_df"
   ],
   "id": "9cd6fe88fa31d01d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simulate random scores in [0, 0.5) to mimic a model that predicts low probabilities for all\n",
    "# np.random.seed(42)\n",
    "y_scores_low = np.random.uniform(0, 0.5, size=100)\n",
    "\n",
    "# New metrics using these random low scores\n",
    "roc_auc_low = roc_auc_score(y_true, y_scores_low)\n",
    "pr_auc_low = average_precision_score(y_true, y_scores_low)\n",
    "\n",
    "# Keep the thresholded class predictions (still all 0s since all probs < 0.5)\n",
    "y_pred_low = (y_scores_low >= 0.5).astype(int)\n",
    "\n",
    "f1_low = f1_score(y_true, y_pred_low, zero_division=0)\n",
    "balanced_acc_low = balanced_accuracy_score(y_true, y_pred_low)\n",
    "precision_low = precision_score(y_true, y_pred_low, zero_division=0)\n",
    "recall_low = recall_score(y_true, y_pred_low, zero_division=0)\n",
    "\n",
    "metrics_df_low = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC AUC', 'PR AUC', 'F1 Score', 'Balanced Accuracy', 'Precision', 'Recall'],\n",
    "    'Value': [\n",
    "        np.mean(y_pred_low == y_true),\n",
    "        roc_auc_low,\n",
    "        pr_auc_low,\n",
    "        f1_low,\n",
    "        balanced_acc_low,\n",
    "        precision_low,\n",
    "        recall_low\n",
    "    ]\n",
    "})\n",
    "\n",
    "metrics_df_low\n"
   ],
   "id": "bdbdf45bcb0ae956"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_scores_low",
   "id": "8f0252d5faa6ff6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76b86d4308b5b048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd8c73b1f39a00cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
